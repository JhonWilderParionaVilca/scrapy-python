<h1 align="center">üï∑ Scraping con Python</h1>
<p align="center"> <a href="https://www.azulschool.net/courses/curso-bases-del-web-scraping-con-python/" target="_blank">Curso de Azul school</a> - rodrigo Urcino </p>

| Tecnolog√≠a | Versi√≥n |
|------------|---------|
| python     | 3.7.3   |
| Scrapy     | 2.5.0   |

## Construccion proyecto

- Crear archivo requirements.txt
- crear un entorno virtual(venv asdf y pycharm)
- dentro del entorno virtual
    - pip3 install scrapy
    - pip3 freeze > requirements.txt
    
## Uso

- crear un env y dentro del entorno
- pip3 install -r requirements.txt

## Que es el Scrapping

> ‚ÄúEl proceso de extracci√≥n de informaci√≥n automatizado de sitios web‚Äù Cabe mencionar que todo esto se basa en HTTP

## Scrapy

> Framework open source

**Conceptos**

- shell: terminal para probar scrapy y python(pre-scrapping)
- selectores: expresiones regulares para seleccionar html o css
- items: da formato de salida
- spiders: extraen los datos estructurados.

## PASOS

1. Inspeccionar la web para saber la estructura del contenido y en que etiquetas se encuentra la informaci√≥n que necesitamos
2. Probar con la shell de scrapy(dentro de nuestro entorno virtual)
```shell
#scrapy shell quotes.toscrape.com
$ scrapy shell <urldelsitio>
# sugar syntax extraer mediante etiquetas y clases css(los hijos se separa con espacio)
$ response.css("div.row div.col-md-8 div.quote span.text::text").getall()
# mediante regex
$ response.xpath("//div[@class='row']//div[@class='col-md-8'] /div[@class='quote']/ span[@class='text']/text()").extract()

```

## Robots.txt

> Revisarlo para definir que puedes ver o que no puedes ver.

- User-Agent: hace referencia al bot que est√° haciendo la petici√≥n al sitio, * (son todos)
- Disallow: las rutas q estan deshabilitados
- Allow: lo que esta permitido(excepcci√≥n, no puedes visualizar lo que esta disallow pero si puedes los q estan en allow)

**Entrar al archivo robots.txt**

En la url del sitio poner `https://sitio.com/robots.txt`
